import os
import sys
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import xgboost as xgb

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from app.data.realistic_data_generator import generate_realistic_training_data


def train_improved_model(n_samples: int = 15000):
    """
    –û–±—É—á–∞–µ—Ç –£–õ–£–ß–®–ï–ù–ù–£–Æ XGBoost –º–æ–¥–µ–ª—å –Ω–∞ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    –¶–µ–ª—å: —Ç–æ—á–Ω–æ—Å—Ç—å 70-80%
    
    Args:
        n_samples: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è 15000+)
    """
    print("=" * 80)
    print("üöÄ –û–±—É—á–µ–Ω–∏–µ –£–õ–£–ß–®–ï–ù–ù–û–ô XGBoost –º–æ–¥–µ–ª–∏ (—Ü–µ–ª—å: 70-80% accuracy)")
    print(f"   üìä –î–∞—Ç–∞—Å–µ—Ç: {n_samples} –†–ï–ê–õ–ò–°–¢–ò–ß–ù–´–• –ø—Ä–∏–º–µ—Ä–æ–≤")
    print("=" * 80)
    
    # –®–∞–≥ 1: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –†–ï–ê–õ–ò–°–¢–ò–ß–ù–´–• –¥–∞–Ω–Ω—ã—Ö
    df = generate_realistic_training_data(
        n_samples=n_samples,
        low_ratio=0.45,     # 45% –Ω–∏–∑–∫–∏–π —Ä–∏—Å–∫
        medium_ratio=0.30,  # 30% —Å—Ä–µ–¥–Ω–∏–π —Ä–∏—Å–∫
        high_ratio=0.25     # 25% –≤—ã—Å–æ–∫–∏–π —Ä–∏—Å–∫
    )
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤
    print("\\nüìà –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:")
    class_counts = df['risk_label'].value_counts().sort_index()
    for label, count in class_counts.items():
        risk_name = {0: 'Low', 1: 'Medium', 2: 'High'}[label]
        print(f"   {risk_name} Risk: {count} ({count/len(df)*100:.1f}%)")
    
    # –®–∞–≥ 2: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    print("\\nüîß –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è...")
    
    feature_columns = [
        'attendance_rate',
        'homework_completion',
        'payment_delays',
        'days_since_last_payment',
        'test_avg_score',
        'communication_activity',
        'days_enrolled',
        'missed_classes_streak'
    ]
    
    X = df[feature_columns].values
    y = df['risk_label'].values
    
    # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train/test (85/15 –¥–ª—è –±–æ–ª—å—à–µ–≥–æ –æ–±—É—á–∞—é—â–µ–≥–æ —Å–µ—Ç–∞)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.15, random_state=42, stratify=y
    )
    
    print(f"   –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_train)} –ø—Ä–∏–º–µ—Ä–æ–≤")
    print(f"   –¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_test)} –ø—Ä–∏–º–µ—Ä–æ–≤")
    
    # –®–∞–≥ 3: –û–±—É—á–µ–Ω–∏–µ –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–û–ô XGBoost –º–æ–¥–µ–ª–∏
    print("\\nü§ñ –û–±—É—á–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π XGBoost –º–æ–¥–µ–ª–∏...")
    
    model = xgb.XGBClassifier(
        n_estimators=300,          # –ë–æ–ª—å—à–µ –¥–µ—Ä–µ–≤—å–µ–≤ –¥–ª—è –ª—É—á—à–µ–π —Ç–æ—á–Ω–æ—Å—Ç–∏
        max_depth=7,               # –ì–ª—É–±–∂–µ –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        learning_rate=0.03,        # –ú–µ–¥–ª–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ = –ª—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å
        min_child_weight=1,        # –ú–µ–Ω—å—à–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π
        subsample=0.85,            # 85% –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –∏—Ç–µ—Ä–∞—Ü–∏—é
        colsample_bytree=0.85,     # 85% –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–∞ –¥–µ—Ä–µ–≤–æ
        gamma=0.05,                # –ú–µ–Ω—å—à–µ gamma = –±–æ–ª—å—à–µ splits
        reg_alpha=0.1,             # L1 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
        reg_lambda=1.0,            # L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
        objective='multi:softprob',
        num_class=3,
        random_state=42,
        verbosity=0,
        early_stopping_rounds=20   # –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –µ—Å–ª–∏ –Ω–µ —É–ª—É—á—à–∞–µ—Ç—Å—è
    )
    
    # –û–±—É—á–µ–Ω–∏–µ —Å validation set –¥–ª—è early stopping
    eval_set = [(X_test, y_test)]
    model.fit(
        X_train, y_train,
        eval_set=eval_set,
        verbose=False
    )
    print("   ‚úÖ –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ —Å early stopping!")
    
    # –®–∞–≥ 4: –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
    print("\\nüìä –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏:")
    
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    
    print(f"\\n   üéØ –¢–æ—á–Ω–æ—Å—Ç—å (Accuracy): {accuracy:.2%}")
    
    print("\\n   üìã Classification Report:")
    target_names = ['Low Risk', 'Medium Risk', 'High Risk']
    print(classification_report(y_test, y_pred, target_names=target_names))
    
    print("   üî¢ Confusion Matrix:")
    cm = confusion_matrix(y_test, y_pred)
    print("      Predicted:  Low  Med  High")
    for i, row in enumerate(cm):
        actual = target_names[i]
        print(f"   {actual:12s}  {row[0]:3d}  {row[1]:3d}  {row[2]:3d}")
    
    # –®–∞–≥ 5: Feature Importance
    print("\\nüìà Feature Importance (–≤–∞–∂–Ω–æ—Å—Ç—å —Ñ–∏—á):")
    importance_dict = dict(zip(feature_columns, model.feature_importances_))
    sorted_importance = sorted(importance_dict.items(), key=lambda x: x[1], reverse=True)
    
    for feature, importance in sorted_importance:
        bar = "‚ñà" * int(importance * 60)
        print(f"   {feature:25s} {importance:.3f} {bar}")
    
    # –®–∞–≥ 6: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    print("\\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏...")
    
    os.makedirs('models/trained', exist_ok=True)
    
    model_path = 'models/trained/churn_model.json'
    model.save_model(model_path)
    
    print(f"   ‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {model_path}")
    print(f"   üì¶ –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {os.path.getsize(model_path) / 1024:.1f} KB")
    
    # –®–∞–≥ 7: –¢–µ—Å—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    print("\\nüß™ –¢–µ—Å—Ç–æ–≤–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ:")
    
    test_cases = [
        {
            'name': '–û—Ç–ª–∏—á–Ω—ã–π —Å—Ç—É–¥–µ–Ω—Ç',
            'data': {
                'attendance_rate': 95.0,
                'homework_completion': 92.0,
                'payment_delays': 0,
                'days_since_last_payment': 5,
                'test_avg_score': 90.0,
                'communication_activity': 18,
                'days_enrolled': 90,
                'missed_classes_streak': 0
            }
        },
        {
            'name': '–ü—Ä–æ–±–ª–µ–º–Ω—ã–π —Å—Ç—É–¥–µ–Ω—Ç',
            'data': {
                'attendance_rate': 35.0,
                'homework_completion': 40.0,
                'payment_delays': 5,
                'days_since_last_payment': 60,
                'test_avg_score': 45.0,
                'communication_activity': 2,
                'days_enrolled': 60,
                'missed_classes_streak': 6
            }
        }
    ]
    
    for case in test_cases:
        X_new = np.array([[case['data'][col] for col in feature_columns]])
        prediction = model.predict(X_new)[0]
        probabilities = model.predict_proba(X_new)[0]
        
        risk_level = {0: 'Low', 1: 'Medium', 2: 'High'}[prediction]
        
        print(f"\\n   {case['name']}:")
        print(f"   ‚û°Ô∏è  –ü—Ä–æ–≥–Ω–æ–∑: {risk_level} Risk (Low: {probabilities[0]:.0%}, Med: {probabilities[1]:.0%}, High: {probabilities[2]:.0%})")
    
    print("\\n" + "=" * 80)
    if accuracy >= 0.70:
        print("‚úÖ –¶–µ–ª—å –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–∞! –¢–æ—á–Ω–æ—Å—Ç—å ‚â• 70%")
    else:
        print(f"‚ö†Ô∏è  –¢–æ—á–Ω–æ—Å—Ç—å {accuracy:.1%} - –Ω—É–∂–Ω–æ –µ—â–µ —É–ª—É—á—à–µ–Ω–∏–µ")
    print("=" * 80)
    print("\\nüí° –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é!")
    print("\\n")


if __name__ == "__main__":
    # –û–±—É—á–∞–µ–º –Ω–∞ 15000 —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö
    train_improved_model(n_samples=15000)
